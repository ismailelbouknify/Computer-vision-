{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aAxo9sCFZldC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qwzeaWRwZoQY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "     Cette fonction permet de charger les poids et le fichier de configuration YoloV3 \n",
    "     à l'aide du module dnn d'OpenCV.\n",
    "     Le fichier coco.names contient les noms des différents objets que notre modèle \n",
    "     a été entraîné à identifier. Nous les stockons dans une liste appelée classes . Maintenant, \n",
    "     pour exécuter une passe en avant en utilisant le module cv2.dnn , nous devons passer les noms \n",
    "     des couches pour lesquelles la sortie doit être calculée. net.getUnconnectedOutLayers () retourne \n",
    "     les indices des couches de sortie du réseau.\n",
    "\"\"\"\n",
    "\n",
    "def load_yolo():\n",
    "    net = cv2.dnn.readNet('yolov3-tiny.weights','yolov3-tiny.cfg')\n",
    "    classes = []\n",
    "    with open('coco.names','r') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "    return net, classes, colors, output_layers_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KNTMd6sjZoTP"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Pour accepter les fichiers image, nous aurons besoin d'une autre fonction appelée load_image () \n",
    "    qui acceptera un chemin d'image comme paramètre, lira l'image et la retournera.\n",
    "\"\"\"\n",
    "def load_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width, _ = img.shape\n",
    "    return img, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SANkS5_6ZoWR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Pour prédire correctement les objets avec des réseaux de neurones profonds, nous devons prétraiter\n",
    "    nos données et le module cv2.dnn nous fournit deux fonctions à cet effet: blobFromImage et blobFromImages. \n",
    "    Ces fonctions effectuent la mise à l'échelle, la soustraction de la moyenne et l'échange de canaux, ce qui est \n",
    "    facultatif. Nous utiliserons blobFromImage dans une fonction appelée detect_objects () qui accepte \n",
    "    les images / images du flux vidéo ou webcam, le modèle et les couches de sortie comme paramètres.\n",
    "    Comme vous pouvez le voir dans l'extrait de code ci-dessus, nous avons utilisé le facteur d' échelle de 1/255,\n",
    "    Par conséquent, nous mettons à l'échelle les pixels de l'image dans la plage de 0 à 1. Il n'y a pas besoin de \n",
    "    soustraction moyenne et c'est pourquoi nous la définissons sur la valeur mean [0, 0, 0] .\n",
    "    La fonction forward () du module cv2.dnn renvoie une liste imbriquée contenant des informations sur tous \n",
    "    les objets détectés qui comprend les coordonnées x et y du centre de l'objet détecté, la hauteur et la largeur \n",
    "    de la boîte englobante, la confiance et les scores pour tous les classes d'objets répertoriés dans \n",
    "    coco.names. La classe avec le score le plus élevé est considérée comme la classe prédite.\n",
    "\"\"\"\n",
    "\n",
    "def detect_objects(img, net, outputLayers):\n",
    "    blob = cv2.dnn.blobFromImage(img, scalefactor=1/255, size=(416, 416), mean=(0,0,0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(outputLayers)\n",
    "    return blob, layerOutputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "z0PlCr2AZoY_"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Dans la fonction get_box_dimensions () , une liste appelée scores est créée qui stocke la confiance \n",
    "    correspondant à chaque objet. Nous identifions ensuite l'indice de classe avec le plus haut niveau de \n",
    "    confiance / score en utilisant np.argmax () . Nous pouvons obtenir le nom de la classe correspondant à l'index \n",
    "    à partir de la liste des classes que nous avons créée dans load_yolo () .\n",
    "    J'ai sélectionné toutes les boîtes englobantes prévues avec une confiance de plus de 50%. Vous pouvez jouer \n",
    "    avec cette valeur.\n",
    "\"\"\"\n",
    "\n",
    "def get_box_dimensions(layerOutputs, height, width):\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0]*width)\n",
    "                center_y = int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "                x = int(center_x - w/2)\n",
    "                y = int(center_y - h/2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append((float(confidence)))\n",
    "                class_ids.append(class_id)\n",
    "    return boxes, confidences, class_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x61lhdUPZocC"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Maintenant que nous avons les sommets de la boîte englobante prédite et class_id (index de la classe d'objets \n",
    "    prédite), nous devons dessiner la boîte englobante et lui ajouter une étiquette d'objet. Nous ferons cela avec\n",
    "    l'aide de la fonction draw_labels () .\n",
    "    cv2.dnn.NMSBoxes () permet d'ajouter, dessiner la boîte englobante et y ajouter une étiquette, Elle permet \n",
    "    aussi de remédier à la situation que certains objets ont été détectés plusieurs fois et que nous avons plus \n",
    "    d'un cadre de délimitation pour un objet. Nous transmettons la valeur seuil de confiance et la valeur \n",
    "    seuil NMS (Suppression non-Maximum) comme paramètres pour sélectionner une boîte englobante. Dans la plage \n",
    "    de 0 à 1, nous devons sélectionner une valeur intermédiaire comme 0,4 ou 0,5 pour nous assurer \n",
    "    que nous détectons les objets qui se chevauchent mais que nous ne finissons pas par obtenir plusieurs cadres \n",
    "    de délimitation pour le même objet.\n",
    "\"\"\"\n",
    "\n",
    "def draw_labels(boxes, confidences, colors, class_ids, classes, img):\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = str(round(confidences[i],2))\n",
    "            color = colors[i]\n",
    "            img = cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "            img = cv2.putText(img, label + \" \" + confidence, (x, y+20), font, 2, color, 2)\n",
    "    cv2.imshow('Image',img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hdIvFkTdeZKe"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Cette fonction image_detect () permet de détecter les objets dans le fichier image spécifié par leur chemin.\n",
    "\"\"\"\n",
    "\n",
    "def image_detect(img_path):\n",
    "    model, classes, colors, output_layers_names = load_yolo()\n",
    "    image, height, width = load_image(img_path)\n",
    "    blob, layerOutputs = detect_objects(image, model, output_layers_names)\n",
    "    boxes, confidences, class_ids = get_box_dimensions(layerOutputs, height, width)\n",
    "    draw_labels(boxes, confidences, colors, class_ids, classes, image)\n",
    "    while True:\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): # Enter q to quit\n",
    "            cv2.destroyAllWindows() # détruire toutes les fenêtres cv2 affichées\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "omscrDfIj07C",
    "outputId": "4b69386e-6f37-4e39-fdff-d646497ef4f2"
   },
   "outputs": [],
   "source": [
    "image_detect('bicycle.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LFPk3sIvmY3I"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    De même, pour l'entrée webcam, nous avons crée la fonction start_webcam () pour le \n",
    "    real-time object detection. Cette fonction retourne l'instance de l'objet de capture d'image avec webcam\n",
    "\"\"\"\n",
    "\n",
    "def start_webcam():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "QoNXOZ3FmY7e",
    "outputId": "78b0ae26-e24d-4965-c234-41d59088a6a9"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Cette fonction permet la détection d'objets en temps réel à l'aide de la webcam du PC\n",
    "\"\"\"\n",
    "\n",
    "def webcam_detect():\n",
    "    model, classes, colors, output_layers = load_yolo()\n",
    "    cap = start_webcam()\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"We cannot open webcam !!!\")\n",
    "    while True:\n",
    "        ret , frame = cap.read()\n",
    "        height, width, channels = frame.shape\n",
    "        blob, outputs = detect_objects(frame, model, output_layers)\n",
    "        boxes, confs, class_ids = get_box_dimensions(outputs, height, width)\n",
    "        draw_labels(boxes, confs, colors, class_ids, classes, frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NaWelxBMmfrd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    De même, pour l'entrée vidéo, nous avons crée la fonction start_video () pour la détection d'objets sur un\n",
    "    fichier vidéo spécifié par leur chemin.\n",
    "\"\"\"\n",
    "\n",
    "def start_video(video_path):\n",
    "    model, classes, colors, output_layers = load_yolo()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        height, width, channels = frame.shape\n",
    "        blob, outputs = detect_objects(frame, model, output_layers)\n",
    "        boxes, confs, class_ids = get_box_dimensions(outputs, height, width)\n",
    "        draw_labels(boxes, confs, colors, class_ids, classes, frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): # Enter q to quit\n",
    "            cv2.destroyAllWindows() # détruire toutes les fenêtres cv2 affichées\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "webcam_detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "XNQgzzaPZn8J",
    "outputId": "e945c9a0-524e-4a3f-e7c1-6122d72b054a"
   },
   "outputs": [],
   "source": [
    "start_video('pedestrians.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Yolov3_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
